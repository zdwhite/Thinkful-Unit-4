{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP capstone Data Pipeline",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zdwhite/Thinkful-Unit-4/blob/master/NLP_capstone_Data_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Va938apWfPvb",
        "colab_type": "code",
        "outputId": "0b85c753-1337-4fdf-9a8c-dcbf6bd2d1f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install praw"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting praw\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/c4/b33aa84d9c5c582d2fd92cb28b7027b5b6285485a68e56c9748cd49dd95b/praw-6.1.1-py2.py3-none-any.whl (117kB)\n",
            "\u001b[K    100% |████████████████████████████████| 122kB 6.1MB/s \n",
            "\u001b[?25hCollecting update-checker>=0.16 (from praw)\n",
            "  Downloading https://files.pythonhosted.org/packages/17/c9/ab11855af164d03be0ff4fddd4c46a5bd44799a9ecc1770e01a669c21168/update_checker-0.16-py2.py3-none-any.whl\n",
            "Collecting websocket-client>=0.54.0 (from praw)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/19/44753eab1fdb50770ac69605527e8859468f3c0fd7dc5a76dd9c4dbd7906/websocket_client-0.56.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K    100% |████████████████████████████████| 204kB 8.7MB/s \n",
            "\u001b[?25hCollecting prawcore<2.0,>=1.0.0 (from praw)\n",
            "  Downloading https://files.pythonhosted.org/packages/76/b5/ce6282dea45cba6f08a30e25d18e0f3d33277e2c9fcbda75644b8dc0089b/prawcore-1.0.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from update-checker>=0.16->praw) (2.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from websocket-client>=0.54.0->praw) (1.11.0)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.16->praw) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.16->praw) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.16->praw) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.16->praw) (2019.3.9)\n",
            "Installing collected packages: update-checker, websocket-client, prawcore, praw\n",
            "Successfully installed praw-6.1.1 prawcore-1.0.1 update-checker-0.16 websocket-client-0.56.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "S7uPfVUwWxMb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import praw\n",
        "import time\n",
        "import pandas as pd\n",
        "import json\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AGUC8HkFehlC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "reddit = praw.Reddit(user_agent='Scrape_bot',\n",
        "                     client_id='_XX9OWL3X9Gv-A', client_secret=\"vAvTdaG9JYkrjd2Pe2_r5Q3wrIo\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qgGidccQ5_QM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "subreddit = reddit.subreddit('Politics')\n",
        "top_subreddit = subreddit.top(limit=50)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J5UHgusM6SkV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2737fa0c-f049-474d-df50-bfc3616fddd0"
      },
      "cell_type": "code",
      "source": [
        "for submission in subreddit.top(limit=1):\n",
        "    print(submission.title, submission.id)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Kim Davis, clerk who refused to sign marriage licenses for gay couples, loses to Democrat 9uuhl3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CrwYTQM56j5r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "submissions_dict = { \"title\":[], \n",
        "                \"score\":[], \n",
        "                \"id\":[], \"url\":[], \n",
        "                \"comms_num\": [], \n",
        "                \"created\": []} \n",
        "                #\"body\":[]}\n",
        "\n",
        "for submission in top_subreddit:\n",
        "    submissions_dict[\"title\"].append(submission.title)\n",
        "    submissions_dict[\"score\"].append(submission.score)\n",
        "    submissions_dict[\"id\"].append(submission.id)\n",
        "    submissions_dict[\"url\"].append(submission.url)\n",
        "    submissions_dict[\"comms_num\"].append(submission.num_comments)\n",
        "    submissions_dict[\"created\"].append(submission.created)\n",
        "    #topics_dict[\"body\"].append(submission.selftext) # ignore self text posts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pm6ge8_Z6sBW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "submissions_data = pd.DataFrame(submissions_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aOicYlyR6uBr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "2f310fda-be52-405d-b5f9-43de966ed6bf"
      },
      "cell_type": "code",
      "source": [
        "submissions_data.head()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comms_num</th>\n",
              "      <th>created</th>\n",
              "      <th>id</th>\n",
              "      <th>score</th>\n",
              "      <th>title</th>\n",
              "      <th>url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2771</td>\n",
              "      <td>1.541553e+09</td>\n",
              "      <td>9uuhl3</td>\n",
              "      <td>101764</td>\n",
              "      <td>Kim Davis, clerk who refused to sign marriage ...</td>\n",
              "      <td>https://www.kentucky.com/news/politics-governm...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20873</td>\n",
              "      <td>1.494882e+09</td>\n",
              "      <td>6bd42j</td>\n",
              "      <td>99354</td>\n",
              "      <td>Trump revealed highly classified information t...</td>\n",
              "      <td>https://www.washingtonpost.com/world/national-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>14557</td>\n",
              "      <td>1.516929e+09</td>\n",
              "      <td>7t12e6</td>\n",
              "      <td>95202</td>\n",
              "      <td>Trump Ordered Mueller Fired, but Backed Off Wh...</td>\n",
              "      <td>https://www.nytimes.com/2018/01/25/us/politics...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6544</td>\n",
              "      <td>1.542671e+09</td>\n",
              "      <td>9ymbzc</td>\n",
              "      <td>89715</td>\n",
              "      <td>Ivanka Trump used a personal email account to ...</td>\n",
              "      <td>https://www.washingtonpost.com/politics/ivanka...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1947</td>\n",
              "      <td>1.511730e+09</td>\n",
              "      <td>7fpqwf</td>\n",
              "      <td>88282</td>\n",
              "      <td>A petition calling for FCC Chairman Ajit Pai t...</td>\n",
              "      <td>https://www.dailydot.com/layer8/we-the-petitio...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   comms_num       created      id   score  \\\n",
              "0       2771  1.541553e+09  9uuhl3  101764   \n",
              "1      20873  1.494882e+09  6bd42j   99354   \n",
              "2      14557  1.516929e+09  7t12e6   95202   \n",
              "3       6544  1.542671e+09  9ymbzc   89715   \n",
              "4       1947  1.511730e+09  7fpqwf   88282   \n",
              "\n",
              "                                               title  \\\n",
              "0  Kim Davis, clerk who refused to sign marriage ...   \n",
              "1  Trump revealed highly classified information t...   \n",
              "2  Trump Ordered Mueller Fired, but Backed Off Wh...   \n",
              "3  Ivanka Trump used a personal email account to ...   \n",
              "4  A petition calling for FCC Chairman Ajit Pai t...   \n",
              "\n",
              "                                                 url  \n",
              "0  https://www.kentucky.com/news/politics-governm...  \n",
              "1  https://www.washingtonpost.com/world/national-...  \n",
              "2  https://www.nytimes.com/2018/01/25/us/politics...  \n",
              "3  https://www.washingtonpost.com/politics/ivanka...  \n",
              "4  https://www.dailydot.com/layer8/we-the-petitio...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "o71SketZdDCI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "submission = [reddit.submission(x) for x in submissions_data['id']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ViiF_7PWeAcn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This function takes a given reddit submission id and does the following:\n",
        "    # cleans comment threads of instances of \"More Comments\"\n",
        "    # gets a list of reddit comment tree ids\n",
        "    # Grabs the body of every Root comment\n",
        "top_comments = []\n",
        "for x in submission[0:50]:\n",
        "  (x).comments.replace_more(limit=0)\n",
        "  comments = list(x.comments)\n",
        "  root_comments = [y.body for y in comments]\n",
        "  top_comments.extend(root_comments)  \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bRzfdlfgclyt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2aa8cbb6-d7c5-4d5c-9e10-494a7f9bf555"
      },
      "cell_type": "code",
      "source": [
        "len(top_comments)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5472"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "w8JwXebnYA3C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "7e4463a6-de52-400c-ab25-513affa3999f"
      },
      "cell_type": "code",
      "source": [
        "# lets start cleaning comments\n",
        "\n",
        "\n",
        "top_comments[0:10]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\nAs a reminder, this subreddit [is for civil discussion.](/r/politics/wiki/index#wiki_be_civil)\\n\\nIn general, be courteous to others. Attack ideas, not users. Personal insults, shill or troll accusations, hate speech, **any** advocating or wishing death/physical harm, and other rule violations can result in a permanent ban. \\n\\nIf you see comments in violation of our rules, please report them.\\n\\n***\\n\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/politics) if you have any questions or concerns.*',\n",
              " 'Good job Kentucky! ',\n",
              " 'title should be: \"Lady who refused to do job, loses job.\"',\n",
              " 'From what I remember, this awful bitch owes the people of Kentucky some goddamn money... And human beings, in general, an apology',\n",
              " \"Guess it was God's will ¯\\\\\\\\\\\\_(ツ)\\\\_/¯\",\n",
              " 'Good, now she can go back to husband number, what was it, four?',\n",
              " '700 votes. Remember every vote matters.',\n",
              " 'It appears that an all powerful god rejected her.',\n",
              " 'Thoughts and prayers.',\n",
              " 'I am thrilled that this relic is being relegated to the dung heap of history.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "metadata": {
        "id": "bE1PJ19i4j5d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Clean the text\n",
        "\n",
        "https://medium.com/ml2vec/using-word2vec-to-analyze-reddit-comments-28945d8cee57\n",
        "\n",
        "https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72"
      ]
    },
    {
      "metadata": {
        "id": "Bd3Zfq-OFH_u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WdNEnsOZWeia",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ccdb48b4-86eb-49eb-81b9-fca58d5979f2"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk.data;\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en', parse=True, tag=True, entity=True)\n",
        "nltk.download('stopwords')\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "from gensim.models import word2vec;\n",
        "\n",
        "from sklearn.cluster import KMeans;\n",
        "from sklearn.neighbors import KDTree;\n",
        "\n",
        "\n",
        "import os\n",
        "import re\n",
        "\n",
        "import logging\n",
        "import time\n",
        "import sys\n",
        "import multiprocessing\n",
        "\n",
        "from wordcloud import WordCloud, ImageColorGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import cycle"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MvC5_-oM40rT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "bf134a89-9634-4dec-934f-82b727d54b08"
      },
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "metadata": {
        "id": "9v045-HJ5AGd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WYh2yqdH6Rk9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Expanding Contractions"
      ]
    },
    {
      "metadata": {
        "id": "ztlmmgna6Uvt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#Contraction Map is located in at the bottom of the book and is a shameless copy of contraction mappings\n",
        "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
        "    \n",
        "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
        "                                      flags=re.IGNORECASE|re.DOTALL)\n",
        "    def expand_match(contraction):\n",
        "        match = contraction.group(0)\n",
        "        first_char = match[0]\n",
        "        expanded_contraction = contraction_mapping.get(match)\\\n",
        "                                if contraction_mapping.get(match)\\\n",
        "                                else contraction_mapping.get(match.lower())                       \n",
        "        expanded_contraction = first_char+expanded_contraction[1:]\n",
        "        return expanded_contraction\n",
        "        \n",
        "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
        "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
        "    return expanded_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iQ3djmZ56UlP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_politics['comments'] = df_politics['comments'].apply(lambda x:expand_contractions(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ivR-UlWW8Nbp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_politics['comments']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "szsVQI7P8pjK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Removing Special characters"
      ]
    },
    {
      "metadata": {
        "id": "Dl73aQwx5BFl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# BIG THANKS \n",
        "cleaned_comments = []\n",
        "for pos in top_comments:\n",
        "\n",
        "    #Get the comment\n",
        "    \n",
        "    #Normalize tabs and remove newlines\n",
        "    no_tabs = str(pos).replace('\\t', ' ').replace('\\n', '');\n",
        "\n",
        "    #Remove all characters except A-Z and a dot.\n",
        "    alphas_only = re.sub(\"[^a-zA-Z\\.]\", \" \", no_tabs);\n",
        "\n",
        "    #Normalize spaces to 1\n",
        "    multi_spaces = re.sub(\" +\", \" \", alphas_only);\n",
        "\n",
        "    #Strip trailing and leading spaces\n",
        "    no_spaces = multi_spaces.strip();\n",
        "\n",
        "    #Normalize all charachters to lowercase\n",
        "    clean_text = no_spaces.lower();\n",
        "\n",
        "    #Get sentences from the tokenizer, remove the dot in each.\n",
        "    sentences = tokenizer.tokenize(clean_text);\n",
        "    sentences = [re.sub(\"[\\.]\", \"\", sentence) for sentence in sentences];\n",
        "    cleaned_comments.extend(sentences)\n",
        "    \n",
        "    #If the text has more than one space (removing single word comments) and one character, write it to the file.\n",
        "    #if len(clean_text) > 0 and clean_text.count(' ') > 0:\n",
        "    #    for sentence in sentences:\n",
        "    #        out_file.write(\"%s\\n\" % sentence)\n",
        "    #        print(sentence);\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hw0eftjy-BpH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "089d5ac2-4969-478b-9566-9b46a922c1ba"
      },
      "cell_type": "code",
      "source": [
        "len(cleaned_comments)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12507"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "metadata": {
        "id": "kB2D4RTl5O-u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "46d4c76f-918a-45a4-94e4-56a15a5d1ef7"
      },
      "cell_type": "code",
      "source": [
        "cleaned_comments[0:10]"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['as a reminder this subreddit is for civil discussion',\n",
              " 'r politics wiki index wiki be civil in general be courteous to others',\n",
              " 'attack ideas not users',\n",
              " 'personal insults shill or troll accusations hate speech any advocating or wishing death physical harm and other rule violations can result in a permanent ban',\n",
              " 'if you see comments in violation of our rules please report them',\n",
              " 'i am a bot and this action was performed automatically',\n",
              " 'please contact the moderators of this subreddit message compose to r politics if you have any questions or concerns',\n",
              " 'good job kentucky',\n",
              " 'title should be lady who refused to do job loses job',\n",
              " 'from what i remember this awful bitch owes the people of kentucky some goddamn money and human beings in general an apology']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "metadata": {
        "id": "TWBT_7Sr0ycp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Lemmatize the text"
      ]
    },
    {
      "metadata": {
        "id": "PokcHUDs5T8p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def lemmatize_text(text):\n",
        "    text = nlp(text)\n",
        "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yvGviv982QP1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_politics = pd.DataFrame(cleaned_comments,columns=['comments'])\n",
        "df_politics['subreddit'] = 'politics'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hE9fToXs2iZR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "efb3e714-3002-4de5-d1ca-43b8cf720479"
      },
      "cell_type": "code",
      "source": [
        "df_politics.head()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comments</th>\n",
              "      <th>subreddit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>as a reminder this subreddit is for civil disc...</td>\n",
              "      <td>politics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>r politics wiki index wiki be civil in general...</td>\n",
              "      <td>politics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>attack ideas not users</td>\n",
              "      <td>politics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>personal insults shill or troll accusations ha...</td>\n",
              "      <td>politics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>if you see comments in violation of our rules ...</td>\n",
              "      <td>politics</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            comments subreddit\n",
              "0  as a reminder this subreddit is for civil disc...  politics\n",
              "1  r politics wiki index wiki be civil in general...  politics\n",
              "2                             attack ideas not users  politics\n",
              "3  personal insults shill or troll accusations ha...  politics\n",
              "4  if you see comments in violation of our rules ...  politics"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "metadata": {
        "id": "JJF-fRGd07gT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_politics['comments']= df_politics['comments'].apply(lambda x : lemmatize_text(x))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "edA2Bqoo5-Pd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Remove Stop Words"
      ]
    },
    {
      "metadata": {
        "id": "1O16tFJ53B3r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text, is_lower_case=False):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0NItPDf-2IKH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_politics['comments'] = df_politics['comments'].apply(lambda x:remove_stopwords(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zv5WL7456CJT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "154c6cb5-3ada-4625-9044-102460cae692"
      },
      "cell_type": "code",
      "source": [
        "df_politics['comments'].head()"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    as a reminder this subreddit be for civil disc...\n",
              "1    r politic wiki index wiki be civil in general ...\n",
              "2                                 attack idea not user\n",
              "3    personal insult shill or troll accusation hate...\n",
              "4    if you see comment in violation of our rule pl...\n",
              "Name: comments, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "metadata": {
        "id": "2IE2Z5mgPnz8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## TFIDF Vectorizer"
      ]
    },
    {
      "metadata": {
        "id": "K6FBb9ZjPsjC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PKGiScYQ8956",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Word 2 Vec"
      ]
    },
    {
      "metadata": {
        "id": "DExqy6yI9CI6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n5U8tvxE6_WJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Other Code"
      ]
    },
    {
      "metadata": {
        "id": "_82ERhAvWyLI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bb_url = 'https://www.reddit.com/r/Politics/.json'\n",
        "bb_headers = {'User-agent': 'Scrape_bot'}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ha-N7mahYazH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bb_res = requests.get(bb_url, headers=bb_headers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PkgIjwWRYtXq",
        "colab_type": "code",
        "outputId": "e16e9da5-8e0d-48d6-ed0a-00b805a35de9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "bb_res.status_code"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "x29BmkWSSTvR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bb_json = bb_res.json()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fa_RQgjUSTvS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bb_json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lnqq0clhSTvV",
        "colab_type": "code",
        "outputId": "52e13824-473d-4d8d-f539-c94f375655da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "sorted(bb_json.keys())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data', 'kind']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "metadata": {
        "id": "_Ykp5XE_cF3N",
        "colab_type": "code",
        "outputId": "5421582a-110e-4f18-9742-1221140ec5af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "sorted(bb_json['data'].keys())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['after', 'before', 'children', 'dist', 'modhash']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "N4ScBdPocGF6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bb_json['data']['children'][0]['data']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7D480QYwcNqx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bb_posts = []\n",
        "bb_after = None\n",
        "#bb_after_list = []\n",
        "\n",
        "for bbp in range(48):\n",
        "    #print(bbp)\n",
        "    if bb_after == None:\n",
        "        bb_params = {}\n",
        "    else:\n",
        "        bb_params = {'after': bb_after}\n",
        "        \n",
        "    bb_res = requests.get(bb_url, params=bb_params, headers=bb_headers)\n",
        "    if bb_res.status_code == 200:\n",
        "        bb_json = bb_res.json()\n",
        "        bb_posts.extend(bb_json['data']['children'])\n",
        "        bb_after = bb_json['data']['after']\n",
        "        #bb_after_list.append(bb_after)\n",
        "\n",
        "    else:\n",
        "        print('Status Code Error', bb_res.status_code)\n",
        "        break\n",
        "    time.sleep(.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Aj5th5RTcUjs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bb_posts "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4le1EufseY6B",
        "colab_type": "code",
        "outputId": "032d6cc7-e06d-40eb-a225-96e376ed44bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "len(submissions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1195"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "TAedgAW5fRMw",
        "colab_type": "code",
        "outputId": "36c7db01-3b40-4a72-8ab7-06b39817f0e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(reddit.read_only)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "67dHZOyEcfcC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "submissions = [reddit.submission(x['data']['id']) for x in bb_posts]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4rcR80Dlfkt8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " submissions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VPRUcg8BhhfV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "top_comments = pd.Dataframe([])\n",
        "for top_level_comment in submissions[1].comments:\n",
        "    (top_level_comment.body)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pvw7_gIBjPEg",
        "colab_type": "code",
        "outputId": "2e20fb11-ba9f-4865-dc38-dacd2ba44a23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "submissions[1].comments"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<praw.models.comment_forest.CommentForest at 0x7f1c590d3e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "metadata": {
        "id": "KthkaTvEjSBw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "parent_comments = []\n",
        "\n",
        "for links in submissions[0:10]:\n",
        "  links.comments.replace_more(limit=0)\n",
        "  for top_level_comments in links.comments:\n",
        "      parent_comments.extend(top_level_comment.body)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rcgjffdljs4x",
        "colab_type": "code",
        "outputId": "bdfc1061-501a-41d8-9f1d-7947bd3ba056",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "submissions[2].comments.replace_more(limit=0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "metadata": {
        "id": "VudGu4Y7kNHL",
        "colab_type": "code",
        "outputId": "879047d5-f586-4c77-f54f-ff85cafe15ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "cell_type": "code",
      "source": [
        "submissions[5].comments.body"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-183522ea80ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msubmissions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'CommentForest' object has no attribute 'body'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "5FH71t7GKXQh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "CONTRACTION_MAP = {\n",
        "\"ain't\": \"is not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he'll've\": \"he he will have\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"I'd\": \"I would\",\n",
        "\"I'd've\": \"I would have\",\n",
        "\"I'll\": \"I will\",\n",
        "\"I'll've\": \"I will have\",\n",
        "\"I'm\": \"I am\",\n",
        "\"I've\": \"I have\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'd've\": \"i would have\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'll've\": \"i will have\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it'll've\": \"it will have\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she'll've\": \"she will have\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so as\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they'll've\": \"they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'd've\": \"we would have\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what'll've\": \"what will have\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"when's\": \"when is\",\n",
        "\"when've\": \"when have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"where've\": \"where have\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who'll've\": \"who will have\",\n",
        "\"who's\": \"who is\",\n",
        "\"who've\": \"who have\",\n",
        "\"why's\": \"why is\",\n",
        "\"why've\": \"why have\",\n",
        "\"will've\": \"will have\",\n",
        "\"won't\": \"will not\",\n",
        "\"won't've\": \"will not have\",\n",
        "\"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"wouldn't've\": \"would not have\",\n",
        "\"y'all\": \"you all\",\n",
        "\"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\",\n",
        "\"y'all're\": \"you all are\",\n",
        "\"y'all've\": \"you all have\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'd've\": \"you would have\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you'll've\": \"you will have\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\"\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}